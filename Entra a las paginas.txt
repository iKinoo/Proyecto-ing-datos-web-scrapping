import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# FunciÃ³n para obtener las noticias de una pÃ¡gina
def get_news(url, mes):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"âŒ Error al acceder a {url}")
        return []
    
    soup = BeautifulSoup(response.text, "html5lib")
    
    news_data = []
    
    # DepuraciÃ³n: Mostrar una parte del HTML para ver si los artÃ­culos estÃ¡n cargando correctamente
    print(f"ğŸ” HTML de la pÃ¡gina {url}:\n{soup.prettify()[:1000]}\n")
    
    # Intentar buscar los artÃ­culos de manera mÃ¡s robusta
    articles = soup.find_all('a', class_='post-headline')
    
    if not articles:
        print(f"âš  No se encontraron artÃ­culos en {mes.capitalize()}")
    
    for i, article in enumerate(articles, start=1):
        try:
            # Buscar el tÃ­tulo
            title = article.text.strip() if article else "Sin tÃ­tulo"
            
            # Obtener el enlace (contenido)
            link = article['href'] if article else "Enlace no encontrado"
            
            # Buscar fecha (utilizando otro selector)
            date_tag = article.find_previous('p', class_='maya')
            date = date_tag.text.strip() if date_tag else "Sin fecha"
            
            # Mostrar datos extraÃ­dos para depuraciÃ³n
            print(f"ğŸ“… {mes.capitalize()} - Noticia {i}")
            print(f"   ğŸ“° TÃ­tulo: {title}")
            print(f"   ğŸ—“ Fecha: {date}")
            print(f"   ğŸ“„ Enlace: {link}\n")
            
            # AÃ±adir al listado de noticias
            news_data.append({'Fecha': date, 'TÃ­tulo': title, 'Contenido': link})
        except AttributeError:
            continue
    
    return news_data

# Lista de meses de 2016
months_2016 = [
    "enero", "febrero", "marzo", "abril", "mayo", "junio",
    "julio", "agosto", "septiembre", "octubre", "noviembre", "diciembre"
]

# URL base
base_url = "https://www.lajornadamaya.mx/k'iintsil/"

# DataFrame para almacenar todas las noticias
df = pd.DataFrame(columns=['Fecha', 'TÃ­tulo', 'Contenido'])

# Iterar sobre cada mes de 2016
for mes in months_2016:
    url = f"{base_url}{mes}-2016"
    print(f"ğŸ” Scrapeando {mes.capitalize()} 2016: {url}")
    news = get_news(url, mes)
    print(f"âœ… Noticias extraÃ­das en este mes: {len(news)}\n")
    
    if news:
        df = pd.concat([df, pd.DataFrame(news)], ignore_index=True)

    time.sleep(2)

# Guardar en CSV
df.to_csv("noticias_2016.csv", index=False, encoding='utf-8-sig')
print("ğŸ‰ Scrapeo completado y guardado en 'noticias_2016.csv'")
